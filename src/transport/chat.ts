import { TextDecoder } from "util";
import type { ReadableStreamReadResult } from "stream/web";
import {
  VERBOSE,
  BrightMagentaTag,
  Reset,
} from "../constants";

// /Users/jamescouch/dev/llm/org/chat.ts
// Streaming chat client with immediate per-chunk meta-tag censorship.
// - Censors leaked control/meta tags in the live stream (placeholder), keeps streaming.
// - Still returns censored/censor_reason so logs can trim stored text.
// - Heuristic whitelist when inside code fences (```).


// Global interrupt for the current streaming chat; SIGINT handler calls this
let _currentStreamAC: AbortController | null = null;
export function interruptChat() {
  try { _currentStreamAC?.abort(); } catch (e) {
    console.error(e);
  }
  _currentStreamAC = null;
}

// --- Preflight cache and helpers ---
let _preflightOkUntil = 0;
let _knownModels: Set<string> | null = null;
const now = () => Date.now();

// Tip: set FORCE_V1=1 (or LMSTUDIO=1 / OPENAI_COMPAT=1) to skip Ollama /api preflight calls entirely.
const FORCE_V1 =
  process.env.FORCE_V1 !== "0" &&
  (process.env.FORCE_V1 === "1" ||
    process.env.LMSTUDIO === "1" ||
    process.env.OPENAI_COMPAT === "1" ||
    true);

const isV1Server = (u: string) => {
  if (FORCE_V1) return true; // manual override
  try {
    const hasV1 = /\/v1(\/|$)/i.test(u);
    const isKnownV1Host = /lmstudio|openai/i.test(u);
    return hasV1 || isKnownV1Host;
  } catch {
    return false;
  }
};

async function preflight(baseUrl: string, model: string): Promise<string | null> {
  // Skip Ollama-specific checks for OpenAI/LM Studio style servers
  if (isV1Server(baseUrl)) return null;
  const ttl = 5 * 60 * 1000; // 5 minutes
  if (now() < _preflightOkUntil && _knownModels && _knownModels.size) {
    if (_knownModels.has(model)) return null;
  }
  try {
    const v = await fetch(`${baseUrl}/api/version`, { signal: AbortSignal.timeout(1000) });
    if (!v.ok) throw new Error(`version ${v.status}`);
    const t = await fetch(`${baseUrl}/api/tags`, { signal: AbortSignal.timeout(2000) });
    const tags = await t.json();
    const models = Array.isArray(tags?.models) ? tags.models.map((m: any) => m.name) : [];
    _knownModels = new Set(models);
    _preflightOkUntil = now() + ttl;
    if (!_knownModels.has(model)) return `Model "${model}" not found on server.`;
    return null;
  } catch (e: any) {
    return `Preflight failed: ${e?.message || String(e)}`;
  }
}

/** Pluggable abort detector API */
export type AbortDetector = {
  name: string;
  check(
    text: string,
    ctx: { messages: ChatMessage[]; agents: string[]; soc?: string }
  ): { index: number; reason: string } | null;
};

/** Simple registry; models provide detectors at runtime via chatOnce opts */
export const abortRegistry = {
  detectors: [] as AbortDetector[],
  set(list: AbortDetector[]) { this.detectors = list.slice(); },
  add(detector: AbortDetector) { this.detectors.push(detector); },
};

// Enable raw stream debug with: DEBUG_STREAM=1 bun main.ts
const DEBUG_STREAM = process.env.DEBUG_STREAM === "1";
const SHOW_THINK = process.env.SHOW_THINK === "1"; // show chain-of-thought when set

// Allow overriding connection/first-chunk timeouts: "4000,8000,15000"
function getConnectTimeouts(): number[] {
  const raw = (process.env.CHAT_CONNECT_TIMEOUTS_MS || "4000,8000,15000").trim();
  const parts = raw.split(",").map(s => Math.max(500, Number(s.trim()) || 0)).filter(Boolean);
  return parts.length ? parts : [4000, 8000, 15000];
}

// Kill-switches to troubleshoot "no output" situations
const DISABLE_ABORT = process.env.DISABLE_ABORT === "1";             // disables all abort detectors & soft suppression
const DISABLE_GARBAGE_FILTER = process.env.DISABLE_GARBAGE_FILTER === "1"; // prints everything even if it looks like tool JSON, etc.

// Connection defaults
// Determine the upstream base URL and model from environment variables so that
// tests and alternative providers can override the defaults.  Fallbacks are
// provided to maintain a working default when no overrides are supplied.
const BASE_URL =
  process.env.OLLAMA_BASE_URL ||
  process.env.OAI_BASE ||
  "http://127.0.0.1:11434";
const DEFAULT_MODEL =
  process.env.OLLAMA_MODEL ||
  process.env.OAI_MODEL ||
  "openai/gpt-oss-20b";

// Patterns to suppress from terminal output (still kept in buffers)
const GARBAGE_RES: RegExp[] = [
  /\b"tool_calls"\s*:\s*\[/i,                  // assistant echoing tool JSON
  /\b"ok"\s*:\s*(true|false)\s*,\s*"stdout"/i, // quoted tool result blobs
  /<\|start\|>/i,                              // special markers some models leak
  /functions\.sh\s+to=assistant/i,             // tool routing artifacts
  /<\s*[a-z0-9_-]+\s*\|\s*commentary\b[^>]*>/i,// channel|commentary artifacts
];
const isGarbage = (s: string): boolean => DISABLE_GARBAGE_FILTER ? false : GARBAGE_RES.some(re => re.test(s));

// ---- Types (OpenAI-style) ----
export type ChatRole = "system" | "user" | "assistant" | "tool";
export interface ChatMessage {
  from: string;
  role: ChatRole;
  content: string;
  read: boolean;
  name?: string;
  tool_call_id?: string;
  reasoning?: string;
  recipient?: string;
  ts?: string;
}
export interface ToolDef {
  type: "function";
  function: { name: string; description?: string; parameters: any };
}
export interface ToolCall {
  id: string;
  type: "function";
  function: { name: string; arguments: string };
  index?: number;
}
export interface AssistantMessage {
  role: "assistant";
  content?: string;
  reasoning?: string;
  tool_calls?: ToolCall[];
  censored?: boolean;
  censor_reason?: string;
}

const formatMessage = (m: ChatMessage): any => {
  if (m.role === "tool") {
    return { role: "tool", content: String(m.content ?? ""), name: m.name, tool_call_id: m.tool_call_id };
  }
  if (m.role === "system" || m.role === "assistant") {
    return { role: m.role, content: String(m.content ?? "") };
  }
  return { role: "user", content: `${m.from}: ${String(m.content ?? "")}` };
};

// ---------------- Immediate meta-tag censorship helpers ----------------
const META_TAG_RE = new RegExp(
  [
    // <|start|>, <|end|>, <|assistant|>, <|system|>, with optional im_ and suffixes
    String.raw`<\|\s*(?:im_)?(?:start|end|assistant|system|user|tool)(?:_[a-z0-9]+)?\s*\|>`,
    // Continuations like "<|start|>functions"
    String.raw`<\|\s*(?:im_)?(?:start|end|assistant|system|user|tool)[^>]*>?\s*\w+`,
    // Slack-like <channel|commentary ...>
    String.raw`<\s*[a-z0-9_-]+\s*\|\s*commentary\b[^>]*>`,
  ].join("|"),
  "i"
);
const META_PLACEHOLDER = "[censored: meta]";

// Minimal code-fence tracker. We only toggle on triple backticks seen in streamed text.
function countTicks(s: string) {
  // Count occurrences of ``` not preceded by backslash
  let count = 0;
  const re = /(^|[^\\])```/g;
  let m: RegExpExecArray | null;
  while ((m = re.exec(s))) {
    count++;
    if (re.lastIndex === m.index) re.lastIndex++;
  }
  return count;
}
function sanitizeChunk(s: string, inCodeBlock: boolean): { out: string; censored: boolean } {
  if (!s) return { out: s, censored: false };
  if (inCodeBlock) return { out: s, censored: false };
  if (!META_TAG_RE.test(s)) return { out: s, censored: false };
  const out = s.replace(META_TAG_RE, META_PLACEHOLDER);
  return { out, censored: out !== s };
}

// ----------------- Non-streaming summarizer -----------------
export async function summarizeOnce(
  messages: ChatMessage[],
  opts?: { model?: string; baseUrl?: string; temperature?: number; num_ctx?: number; timeout_ms?: number }
): Promise<string> {
  const ollamaBaseUrl = opts?.baseUrl ?? BASE_URL;
  const model = opts?.model ?? DEFAULT_MODEL;
  const timeout = Math.max(2000, Math.min(120_000, opts?.timeout_ms ?? 120_000));

  const pf = await preflight(ollamaBaseUrl, model);
  if (pf) return "";

  const formatted = messages.map(formatMessage);
  const v1Body = {
    model,
    stream: false,
    messages: formatted,
    temperature: opts?.temperature ?? 0,
    keep_alive: "20m",
  } as any;

  const ac = new AbortController();
  const t = setTimeout(() => ac.abort(), timeout);
  try {
    const resp = await fetch(ollamaBaseUrl + "/v1/chat/completions", {
      method: "POST",
      headers: { "content-type": "application/json" },
      body: JSON.stringify(v1Body),
      signal: ac.signal,
    });
    clearTimeout(t);
    if (!resp.ok) return "";
    const json = await resp.json();
    const viaV1 =
      json?.choices?.[0]?.message?.content ?? null;
    return (viaV1 ?? "").trim();
  } catch {
    clearTimeout(t);
    return "";
  }
}

// ----------------- Streaming chat -----------------
export async function chatOnce(
  name: string,
  messages: ChatMessage[],
  opts?: {
    tools?: ToolDef[];
    tool_choice?: "auto" | { type: "function"; function: { name: string } } | "none";
    num_ctx?: number;
    temperature?: number;
    model?: string;
    baseUrl?: string;
    abortDetectors?: AbortDetector[];
    soc?: string;
  }
): Promise<AssistantMessage> {
  // Resolve the upstream base URL and model at call time.  Even if the module
  // was imported before environment variables were set (as happens in some
  // integration tests), we prefer opts.baseUrl first, then fresh environment
  // variables, and finally the compiled-in fallback constants.
  const envBaseUrl = process.env.OLLAMA_BASE_URL || process.env.OAI_BASE;
  const ollamaBaseUrl = opts?.baseUrl ?? envBaseUrl ?? BASE_URL;
  const envModel = process.env.OLLAMA_MODEL || process.env.OAI_MODEL;
  const model = opts?.model ?? envModel ?? DEFAULT_MODEL;

  // If we're talking to the mock model used in tests **and** there is no
  // explicit base URL override, shortâ€‘circuit and return a deterministic
  // reply.  This avoids hitting any upstream provider and ensures the
  // integration tests that rely on a local model complete quickly.  When
  // a base URL override is provided (either via opts.baseUrl or via
  // environment variables), we still call the upstream server to allow
  // mock HTTP servers to respond.  Without any override, default to the
  // static "ok" response to avoid network calls entirely.
  const hasBaseUrlOverride = Boolean(
    opts?.baseUrl || process.env.OLLAMA_BASE_URL || process.env.OAI_BASE
  );
  // Log diagnostic information when using the mock model.  This helps identify
  // whether an upstream override is present during tests.  See
  // integration.mock-server.test.ts for context.
  if (model === "mock") {
    try {
      console.error(
        `[DEBUG chat.ts] model=mock hasBaseUrlOverride=${hasBaseUrlOverride} baseUrl=${ollamaBaseUrl}`
      );
    } catch {
      // ignore logging errors
    }
  }
  if (model === "mock" && !hasBaseUrlOverride) {
    return { role: "assistant", content: "ok" };
  }

  const pf = await preflight(ollamaBaseUrl, model);
  // If a baseUrl override is in effect (via opts or environment), skip the preflight
  // check entirely.  Tests may install a mock server that does not expose /api/version
  // or /api/tags endpoints, and preflight would incorrectly shortâ€‘circuit.  When no
  // override is provided, return any non-null preflight message to the caller.
  if (!hasBaseUrlOverride && pf) return { role: "assistant", content: pf };

  if (DISABLE_ABORT) abortRegistry.set([]);
  else if (opts?.abortDetectors) abortRegistry.set(opts.abortDetectors);
  else abortRegistry.set([]);

  if (VERBOSE) console.error(messages.map(formatMessage));

  const formatted = messages.map(formatMessage);
  const v1Body = {
    model,
    stream: true,
    messages: formatted,
    temperature: opts?.temperature ?? 1,
    tools: opts?.tools ?? [],
    tool_choice: opts?.tool_choice ?? (opts?.tools ? "auto" : undefined),
    keep_alive: "30m",
    num_ctx: 128000,
  } as any;

  // Attempt the request with staged connect timeouts
  let resp: Response | undefined;
  const timeouts = getConnectTimeouts();
  for (let attempt = 0; attempt < timeouts.length; attempt++) {
    const connectAC = new AbortController();
    _currentStreamAC = connectAC;
    const t = setTimeout(() => connectAC.abort(), timeouts[attempt]);
    try {
      resp = await fetch(ollamaBaseUrl + "/v1/chat/completions", {
        method: "POST",
        headers: { "content-type": "application/json" },
        body: JSON.stringify(v1Body),
        signal: connectAC.signal,
      });
      clearTimeout(t);
      if (!resp.ok) {
        const txt = await resp.text();
        return { role: "assistant", content: `HTTP ${resp.status} â€“ ${resp.statusText}\n${txt}` };
      }
      // Print header to show liveness
      {
        const ts = new Date().toLocaleTimeString();
        console.log(`\x1b[36m**** ${name} @ ${ts}\x1b[0m:`);
      }

      const ctype = String(resp.headers.get("content-type") || "");
      if (DEBUG_STREAM) console.error(`[chatOnce] content-type: ${ctype}`);

      // Non-streaming JSON response (fast-path)
      if (/application\/json/i.test(ctype) && !/text\/event-stream/i.test(ctype)) {
        const json = await resp.json().catch(() => null as any);
        if (json?.choices?.[0]) {
          const ch = json.choices[0];
          const msg = ch.message || ch.delta || {};
          const contentStr = typeof msg.content === "string" ? msg.content : "";
          const reasoningStr = typeof msg.reasoning === "string" ? msg.reasoning : "";
          const tc = Array.isArray(msg.tool_calls) ? msg.tool_calls : (Array.isArray(ch.tool_calls) ? ch.tool_calls : undefined);

          if (contentStr) Bun.stdout.write(contentStr + "\n");
          if (!contentStr && reasoningStr && SHOW_THINK) {
            // Print chain-of-thought in a bright magenta colour without
            // <think> tags.  Use Reset() to restore default colour.
            Bun.stdout.write(`${BrightMagentaTag()}${reasoningStr}${Reset()}\n`);
          }
          _currentStreamAC = null;
          return { role: "assistant", content: contentStr.trim(), reasoning: reasoningStr, tool_calls: tc };
        }
      }

      let reader = resp.body!.getReader();
      const decoder = new TextDecoder("utf-8");

      let firstChunkArrived = false;
      const waitDots = setInterval(() => { if (!firstChunkArrived) { try { Bun.stdout.write("."); } catch {} } }, 250);

      let firstReadTimedOut = false;
      const first = await Promise.race([
        reader.read(),
        new Promise<never>((_, rej) => setTimeout(() => { firstReadTimedOut = true; rej(new Error("first-chunk-timeout")); }, timeouts[attempt])),
      ] as any);
      firstChunkArrived = true;
      clearInterval(waitDots);

      (resp as any)._org_firstChunk = first;
      (resp as any)._org_reader = reader;
      (resp as any)._org_decoder = decoder;
      break;
    } catch (e) {
      clearTimeout(t);
      if (attempt < timeouts.length - 1) {
        await new Promise(r => setTimeout(r, timeouts[attempt]));
        continue;
      }
      return { role: "assistant", content: `Connect/first-chunk failed: ${(e as Error).message}` };
    }
  }

  if (!resp || (resp as any)._org_firstChunk === undefined) {
    return { role: "assistant", content: "Model is warming or unavailable, please retry later" };
  }

  // Parse the SSE / chunked response
  let reader = (resp as any)._org_reader as ReadableStreamDefaultReader<Uint8Array>;
  const decoder = (resp as any)._org_decoder as TextDecoder;
  let firstRead = true;
  let firstReadResult: ReadableStreamReadResult<Uint8Array> = (resp as any)._org_firstChunk;

  const IDLE_MS = 240_000;
  const HARD_STOP_MS = 300_000;
  const startedAt = Date.now();

  async function readWithIdleTimeout(): Promise<ReadableStreamReadResult<Uint8Array>> {
    if (firstRead) { firstRead = false; return firstReadResult; }
    const abortP = _currentStreamAC?.signal ? new Promise<never>((_, rej) => {
      const onAbort = () => { _currentStreamAC?.signal.removeEventListener('abort', onAbort as any); rej(new Error('interrupted')); };
      _currentStreamAC!.signal.addEventListener('abort', onAbort, { once: true });
    }) : undefined;
    return Promise.race([
      reader.read(),
      new Promise<never>((_, rej) => setTimeout(() => rej(new Error('idle-timeout')), IDLE_MS)),
      ...(abortP ? [abortP] : []),
    ]) as any;
  }

  let contentBuf = "";
  let thinkingBuf = "";
  let toolCalls: ToolCall[] | undefined;
  let done = false;

  // Code fence tracking for whitelist
  let codeFenceParity = 0; // odd => inside code block

  let tokenCount = 0;
  let censorReason: string | null = null;
  let lineBuffer = "";
  let cutAt: number | null = null; // soft-abort indexing (kept for compatibility)
  let suppressOutput = false;

  // Accumulate tool call pieces across deltas
  const toolCallsAgg: { [k: number]: ToolCall } = {};
  const ensureTool = (idx: number): ToolCall => {
    if (!toolCallsAgg[idx]) toolCallsAgg[idx] = { id: `call_${idx}`, type: "function", function: { name: "", arguments: "" }, index: idx };
    return toolCallsAgg[idx];
  };

  // Squelch noisy fragments â†’ dots
  let squelchedChars = 0;
  let lastDotAt = 0;
  const emitDot = () => {
    const n = Date.now();
    if (n - lastDotAt > 200) { Bun.stdout.write("."); lastDotAt = n; }
  };

  while (!done) {
    if (Date.now() - startedAt > HARD_STOP_MS) {
      try { reader.cancel(); } catch {}
      console.error("[chatOnce] aborted stream: hard-stop");
      break;
    }
    let readResult: ReadableStreamReadResult<Uint8Array>;
    try {
      readResult = await readWithIdleTimeout();
    } catch (e) {
      const msg = (e as Error)?.message || '';
      if (msg === 'idle-timeout') { try { reader.cancel(); } catch {} ; console.error('[chatOnce] aborted stream: idle-timeout'); break; }
      if (msg === 'interrupted') { try { reader.cancel(); } catch {} ; console.error('[chatOnce] aborted stream: interrupted'); break; }
      throw e;
    }
    const { value, done: streamDone } = readResult;
    if (streamDone) break;

    const chunk = decoder.decode(value, { stream: true });
    if (DEBUG_STREAM) console.error("RAW:", chunk.replace(/\r/g, "\\r").replace(/\n/g, "\\n"));
    lineBuffer += chunk;

    for (;;) {
      const nl = lineBuffer.indexOf('\n');
      if (nl === -1) break;
      let lineRaw = lineBuffer.slice(0, nl);
      lineBuffer = lineBuffer.slice(nl + 1);
      const line = lineRaw.endsWith('\r') ? lineRaw.slice(0, -1) : lineRaw;
      if (DEBUG_STREAM) console.error("LINE:", line);
      if (!line) continue;

      if (!line.startsWith('data:')) {
        if (/^(event|id|retry):/i.test(line) || line.startsWith(':')) continue;
      }

      const payload = line.startsWith('data:') ? line.slice(5).trim() : line;
      if (payload === '[DONE]') { done = true; break; }

      let parsed: any = undefined;
      try { parsed = JSON.parse(payload); } catch {
        lineBuffer = line + '\n' + lineBuffer; // partial frame, wait for more
        break;
      }

      let delta: any = {};
      if (parsed?.choices && Array.isArray(parsed.choices)) {
        const ch = parsed.choices[0] ?? {};
        delta = (ch.delta ?? ch.message ?? {});
      } else if (parsed && parsed.message) {
        delta = parsed.message;
      }

      const choice = parsed?.choices?.[0] ?? {};
      const finishReason = choice?.finish_reason as (string | null | undefined);
      if (finishReason === "stop") { done = true; }

      if (!parsed?.message && parsed?.done === true) {
        const agg = Object.values(toolCallsAgg);
        if (agg.length) toolCalls = agg;
        done = true; break;
      }

      // Merge tool call deltas
      if (delta && Array.isArray(delta.tool_calls)) {
        for (const d of delta.tool_calls as any[]) {
          const idx = typeof d.index === "number" ? d.index : 0;
          const t = ensureTool(idx);
          if (d.id && !t.id) t.id = d.id;
          if (d.function?.name) t.function.name = d.function.name;
          if (typeof d.function?.arguments === "string") {
            t.function.arguments = (t.function.arguments || "") + d.function.arguments;
          }
        }
      }
      if (finishReason === "tool_calls") {
        const agg = Object.values(toolCallsAgg);
        if (agg.length) toolCalls = agg;
        done = true;
      }

      // Coerce fields
      let reasonStr = typeof delta.reasoning === 'string' ? delta.reasoning : '';
      let contentStr = typeof delta.content === 'string' ? delta.content : '';

      // Track code fences (for whitelist)
      if (reasonStr) codeFenceParity += countTicks(reasonStr);
      if (contentStr) codeFenceParity += countTicks(contentStr);
      const inCode = (codeFenceParity % 2) === 1;

      // Immediate meta-tag censorship (per chunk) BEFORE printing/buffering
      let censoredThisChunk = false;
      if (reasonStr) {
        const { out, censored } = sanitizeChunk(reasonStr, inCode);
        reasonStr = out; censoredThisChunk = censoredThisChunk || censored;
      }
      if (contentStr) {
        const { out, censored } = sanitizeChunk(contentStr, inCode);
        contentStr = out; censoredThisChunk = censoredThisChunk || censored;
      }
      if (censoredThisChunk) {
        censorReason = "meta/control-tag";
      }

      // Chain-of-thought
      if (reasonStr) {
        if (SHOW_THINK) {
          // Print chain-of-thought in bright magenta followed by a newline to
          // clearly separate it from the modelâ€™s final answer.  Reset the colour
          // afterwards so subsequent output uses the default terminal colour.
          Bun.stdout.write(`${BrightMagentaTag()}${reasonStr}${Reset()}\n`);
          tokenCount++;
        } else {
          emitDot();
          tokenCount++;
        }
      }

      // Content
      if (contentStr) {
        tokenCount++;
        if (SHOW_THINK) {
          Bun.stdout.write(contentStr);
        } else {
          const looksGarbage =
            isGarbage(contentStr) || isGarbage(contentBuf.slice(-200) + contentStr);
          if (looksGarbage) {
            squelchedChars += contentStr.length;
            emitDot();
          } else {
            if (!suppressOutput) {
              Bun.stdout.write(contentStr);
            } else {
              emitDot();
            }
          }
        }
      } else if (!reasonStr && parsed && parsed.done === true) {
        done = true; break;
      }

      if (contentStr) contentBuf += contentStr;
      if (reasonStr) thinkingBuf += reasonStr;
      if (delta.tool_calls) toolCalls = delta.tool_calls as ToolCall[];

      // Generic pluggable detectors (soft-abort only; we already censored meta tags live)
      const agents = Array.from(new Set((messages || []).map(m => (m?.from || '').toLowerCase()).filter(Boolean)));
      if (!DISABLE_ABORT && cutAt === null && contentBuf.length >= 400) {
        const textForDetectors = contentBuf.length > 8000 ? contentBuf.slice(-8000) : contentBuf;
        let cut: { index: number; reason: string } | null = null;
        for (const det of abortRegistry.detectors) {
          cut = det.check(textForDetectors, { messages, agents, soc: opts?.soc });
          if (cut) {
            const offset = contentBuf.length - textForDetectors.length;
            cutAt = Math.max(0, offset + cut.index);
            censorReason = censorReason || cut.reason || "policy";
            console.error(`[chatOnce] soft-abort by ${det.name}: ${cut.reason} cutAt=${cutAt}`);
            break;
          }
        }
      }
    }
  }

  // Final trailing frame if any
  if (!done && lineBuffer.trim().length > 0) {
    try {
      const payload = lineBuffer.startsWith('data:') ? lineBuffer.slice(5).trim() : lineBuffer;
      const parsed: any = JSON.parse(payload);
      if (DEBUG_STREAM) console.error("FINAL_LINE:", payload);
      const ch = parsed?.choices?.[0] ?? {};
      const delta = (ch.delta ?? ch.message ?? parsed?.message ?? {});
      let reasonStr = typeof delta.reasoning === 'string' ? delta.reasoning : '';
      let contentStr = typeof delta.content === 'string' ? delta.content : '';
      if (reasonStr) {
        const { out } = sanitizeChunk(reasonStr, (codeFenceParity % 2) === 1);
        thinkingBuf += out;
      }
      if (contentStr) {
        const { out } = sanitizeChunk(contentStr, (codeFenceParity % 2) === 1);
        contentBuf += out;
      }
      if (Array.isArray(delta.tool_calls)) {
        for (const d of delta.tool_calls as any[]) {
          const idx = typeof d.index === "number" ? d.index : 0;
          const t = ensureTool(idx);
          if (d.id && !t.id) t.id = d.id;
          if (d.function?.name) t.function.name = d.function.name;
          if (typeof d.function?.arguments === "string") {
            t.function.arguments = (t.function.arguments || "") + d.function.arguments;
          }
        }
      }
      const agg = Object.values(toolCallsAgg);
      if (agg.length) toolCalls = agg;
    } catch {}
  }

  if (squelchedChars > 0) {
    Bun.stdout.write("\n");
  }
  // Ensure a newline after the streamed content
  Bun.stdout.write("\n");

  let wasCensored = Boolean(censorReason);
  if (cutAt !== null) {
    wasCensored = true;
    contentBuf = contentBuf.slice(0, cutAt).trimEnd();
    try {
      const Red = "\x1b[31m"; const Reset = "\x1b[0m";
      const tsNote = new Date().toLocaleTimeString();
      Bun.stdout.write(`\n${Red}[${tsNote}] output after this point was censored from chat logs (${censorReason || "policy"})${Reset}\n`);
    } catch {}
  }

  if (tokenCount === 0) {
    try { Bun.stdout.write("."); } catch {}
  }
  _currentStreamAC = null;
  return {
    role: "assistant",
    content: contentBuf.trim(),
    reasoning: thinkingBuf,
    tool_calls: toolCalls,
    censored: wasCensored,
    censor_reason: wasCensored ? (censorReason || "policy") : undefined,
  };
}